{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Raising-errors-on-unknown-marks:-–strict\" data-toc-modified-id=\"Raising-errors-on-unknown-marks:-–strict-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Raising errors on unknown marks: –strict</a></span></li><li><span><a href=\"#Marker-revamp-and-iteration\" data-toc-modified-id=\"Marker-revamp-and-iteration-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Marker revamp and iteration</a></span><ul class=\"toc-item\"><li><span><a href=\"#Updating-code\" data-toc-modified-id=\"Updating-code-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Updating code</a></span></li><li><span><a href=\"#Future-updates\" data-toc-modified-id=\"Future-updates-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Future updates</a></span></li></ul></li><li><span><a href=\"#Marking-test-functions-and-selecting-them-for-a-run\" data-toc-modified-id=\"Marking-test-functions-and-selecting-them-for-a-run-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Marking test functions and selecting them for a run</a></span></li><li><span><a href=\"#Selecting-tests-based-on-their-node-ID\" data-toc-modified-id=\"Selecting-tests-based-on-their-node-ID-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Selecting tests based on their node ID</a></span><ul class=\"toc-item\"><li><span><a href=\"#Node-ID-format\" data-toc-modified-id=\"Node-ID-format-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Node ID format</a></span></li></ul></li><li><span><a href=\"#Using--k-expr-to-select-tests-based-on-their-name\" data-toc-modified-id=\"Using--k-expr-to-select-tests-based-on-their-name-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Using -k expr to select tests based on their name</a></span><ul class=\"toc-item\"><li><span><a href=\"#Problem-names\" data-toc-modified-id=\"Problem-names-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Problem names</a></span></li><li><span><a href=\"#Searching-for-parametrizations\" data-toc-modified-id=\"Searching-for-parametrizations-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Searching for parametrizations</a></span></li></ul></li><li><span><a href=\"#Registering-markers\" data-toc-modified-id=\"Registering-markers-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Registering markers</a></span></li><li><span><a href=\"#Marking-whole-classes-or-modules\" data-toc-modified-id=\"Marking-whole-classes-or-modules-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Marking whole classes or modules</a></span></li><li><span><a href=\"#Marking-individual-tests-when-using-parametrize\" data-toc-modified-id=\"Marking-individual-tests-when-using-parametrize-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Marking individual tests when using parametrize</a></span><ul class=\"toc-item\"><li><span><a href=\"#Working-with-single-callables\" data-toc-modified-id=\"Working-with-single-callables-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Working with single callables</a></span></li></ul></li><li><span><a href=\"#Custom-marker-and-command-line-option-to-control-test-runs\" data-toc-modified-id=\"Custom-marker-and-command-line-option-to-control-test-runs-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Custom marker and command line option to control test runs</a></span></li><li><span><a href=\"#Passing-a-callable-to-custom-markers\" data-toc-modified-id=\"Passing-a-callable-to-custom-markers-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Passing a callable to custom markers</a></span></li><li><span><a href=\"#Reading-markers-which-were-set-from-multiple-places\" data-toc-modified-id=\"Reading-markers-which-were-set-from-multiple-places-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Reading markers which were set from multiple places</a></span></li><li><span><a href=\"#Marking-platform-specific-tests-with-pytest\" data-toc-modified-id=\"Marking-platform-specific-tests-with-pytest-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Marking platform specific tests with pytest</a></span></li><li><span><a href=\"#Automatically-adding-markes-based-on-test-names\" data-toc-modified-id=\"Automatically-adding-markes-based-on-test-names-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;</span>Automatically adding markes based on test names</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marking test funtions with attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Marking test functions with attributes](https://docs.pytest.org/en/latest/mark.html#marking-test-functions-with-attributes)\n",
    "\n",
    "This tutorial uses `4_marks` as a root directory, mostly with `test_server.py`\n",
    "\n",
    "The `pytest.mark` helper allows metadata to be set on test functions. Builtin markers (note custom markers can also be created):\n",
    "- `skip`: always skip a test\n",
    "- `skipif`: conditionally skip a test\n",
    "- `xfail`: produce \"expected failure\" under certain conditions\n",
    "- `parametrize`: perform multiple calls to same function\n",
    "\n",
    "Additional [custom markers](https://docs.pytest.org/en/latest/mark.html#marking-test-functions-with-attributes) documentation provides examples and tutorial examples are documented below beginning at [Marking test functions and selecting them for a run](#Marking-test-functions-and-selecting-them-for-a-run)\n",
    "\n",
    "**Note**: Marks can only be applied to tests, not fixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Raising errors on unknown marks: –strict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[Raising errors on unknown marks: –strict](https://docs.pytest.org/en/latest/mark.html#raising-errors-on-unknown-marks-strict)\n",
    "\n",
    "When `--strict` command-line flag is passed, any marks not in `pytest.ini` will trigger an error\n",
    "\n",
    "Marks can be registered inside `pytest.ini` as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# content of pytest.ini\n",
    "[pytest]\n",
    "markers =\n",
    "    slow\n",
    "    serial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To enforce that mark names are not accidentally mistyped, the `addopts = --strict` field can be added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "[pytest]\n",
    "addopts = --strict\n",
    "markers =\n",
    "    slow\n",
    "    serial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Marker revamp and iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[Marker revamp and iteration](https://docs.pytest.org/en/latest/mark.html#marker-revamp-and-iteration)\n",
    "\n",
    "pytest version 3.6 has updated the mechanics of markers, so some old pytest code may need to be updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Updating code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[Updating code](https://docs.pytest.org/en/latest/mark.html#marker-revamp-and-iteration)\n",
    "\n",
    "The old `Node.get_marker(name)` function is deprecated. Thus, there must be replacements as follows for old code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# replace this:\n",
    "marker = item.get_marker(\"log_level\")\n",
    "if marker:\n",
    "    level = marker.args[0]\n",
    "\n",
    "# by this:\n",
    "marker = item.get_closest_marker(\"log_level\")\n",
    "if marker:\n",
    "    level = marker.args[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Additionally, the `skipif(condition)` mark may need to be refactored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# replace this\n",
    "skipif = item.get_marker(\"skipif\")\n",
    "if skipif:\n",
    "    for condition in skipif.args:\n",
    "        # eval condition\n",
    "        ...\n",
    "\n",
    "# by this:\n",
    "for skipif in item.iter_markers(\"skipif\"):\n",
    "    condition = skipif.args[0]\n",
    "    # eval condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Future updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Class-based markers will be introduced in the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Marking test functions and selecting them for a run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[Marking test functions and selecting them for a run](https://docs.pytest.org/en/latest/example/markers.html#marking-test-functions-and-selecting-them-for-a-run)\n",
    "\n",
    "A test function can be marked with metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# content of test_server.py\n",
    "\n",
    "import pytest\n",
    "@pytest.mark.webtest\n",
    "def test_send_http():\n",
    "    pass # perform some webtest test for your app\n",
    "def test_something_quick():\n",
    "    pass\n",
    "def test_another():\n",
    "    pass\n",
    "class TestClass(object):\n",
    "    def test_method(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Then to only run tests marked `webtest`:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$ pytest -v -m webtest\n",
    "============================= test session starts =============================\n",
    "platform win32 -- Python 3.7.1, pytest-3.9.1, py-1.7.0, pluggy-0.8.0 -- C:\\Users\\kahna\\AppData\\Local\\Continuum\\anaconda3\\envs\\tutorials\\python.exe\n",
    "cachedir: .pytest_cache\n",
    "rootdir: C:\\Users\\kahna\\Documents\\Code\\python-intermediate-tutorials\\pytest_official_tutorial, inifile:\n",
    "collected 4 items / 3 deselected\n",
    "\n",
    "test_server.py::test_send_http PASSED                                    [100%]\n",
    "\n",
    "=================== 1 passed, 3 deselected in 0.02 seconds ===================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To run all tests not marked `webtest`:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$ pytest -v -m \"not webtest\"\n",
    "============================= test session starts =============================\n",
    "platform win32 -- Python 3.7.1, pytest-3.9.1, py-1.7.0, pluggy-0.8.0 -- C:\\Users\\kahna\\AppData\\Local\\Continuum\\anaconda3\\envs\\tutorials\\python.exe\n",
    "cachedir: .pytest_cache\n",
    "rootdir: C:\\Users\\kahna\\Documents\\Code\\python-intermediate-tutorials\\pytest_official_tutorial, inifile:\n",
    "collected 4 items / 1 deselected\n",
    "\n",
    "test_server.py::test_something_quick PASSED                              [ 33%]\n",
    "test_server.py::test_another PASSED                                      [ 66%]\n",
    "test_server.py::TestClass::test_method PASSED                            [100%]\n",
    "\n",
    "=================== 3 passed, 1 deselected in 0.02 seconds ===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Selecting tests based on their node ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[Selecting tests based on their node ID](https://docs.pytest.org/en/latest/example/markers.html#selecting-tests-based-on-their-node-id)\n",
    "\n",
    "See `test_server.py`\n",
    "\n",
    "[Node IDs](https://docs.pytest.org/en/latest/example/markers.html#node-id) can be provided to select only specified tests based on module, class, method, or function name:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$ pytest -v test_server.py::TestClass::test_method\n",
    "============================= test session starts =============================\n",
    "platform win32 -- Python 3.7.1, pytest-3.9.1, py-1.7.0, pluggy-0.8.0 -- C:\\Users\\kahna\\AppData\\Local\\Continuum\\anaconda3\\envs\\tutorials\\python.exe\n",
    "cachedir: .pytest_cache\n",
    "rootdir: C:\\Users\\kahna\\Documents\\Code\\python-intermediate-tutorials\\pytest_official_tutorial, inifile:\n",
    "collected 1 item\n",
    "\n",
    "test_server.py::TestClass::test_method PASSED                            [100%]\n",
    "\n",
    "========================== 1 passed in 0.02 seconds ==========================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Additionally, the class alone can be selected:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$ pytest -v test_server.py::TestClass\n",
    "============================= test session starts =============================\n",
    "platform win32 -- Python 3.7.1, pytest-3.9.1, py-1.7.0, pluggy-0.8.0 -- C:\\Users\\kahna\\AppData\\Local\\Continuum\\anaconda3\\envs\\tutorials\\python.exe\n",
    "cachedir: .pytest_cache\n",
    "rootdir: C:\\Users\\kahna\\Documents\\Code\\python-intermediate-tutorials\\pytest_official_tutorial, inifile:\n",
    "collected 1 item\n",
    "\n",
    "test_server.py::TestClass::test_method PASSED                            [100%]\n",
    "\n",
    "========================== 1 passed in 0.02 seconds ==========================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Multiple nodes can be selected:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$ pytest -v test_server.py::TestClass test_server.py::test_send_http\n",
    "============================= test session starts =============================\n",
    "platform win32 -- Python 3.7.1, pytest-3.9.1, py-1.7.0, pluggy-0.8.0 -- C:\\Users\\kahna\\AppData\\Local\\Continuum\\anaconda3\\envs\\tutorials\\python.exe\n",
    "cachedir: .pytest_cache\n",
    "rootdir: C:\\Users\\kahna\\Documents\\Code\\python-intermediate-tutorials\\pytest_official_tutorial, inifile:\n",
    "collected 2 items\n",
    "\n",
    "test_server.py::TestClass::test_method PASSED                            [ 50%]\n",
    "test_server.py::test_send_http PASSED                                    [100%]\n",
    "\n",
    "========================== 2 passed in 0.03 seconds ==========================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Node ID format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Node IDs are of the form `module.py::class:method` or `module.py::function`\n",
    "\n",
    "`module.py::class` will select all test methods in a class\n",
    "\n",
    "Selecting a parametrized test must include the parameter value, like `module.py::function[param]`\n",
    "\n",
    "Node IDs for failing tests are displayed in the test summary only when running with `-rf`\n",
    "\n",
    "Node IDs can be generated from pytest output via `--collectonly`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Using -k expr to select tests based on their name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[Using -k expr to select tests based on their name](https://docs.pytest.org/en/latest/example/markers.html#using-k-expr-to-select-tests-based-on-their-name)\n",
    "\n",
    "The `-k` command line option specifies an expression that implements a substring match on test names rather than the exact match provided by `-m`:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$ pytest -v -k http\n",
    "============================= test session starts =============================\n",
    "platform win32 -- Python 3.7.1, pytest-3.9.1, py-1.7.0, pluggy-0.8.0 -- C:\\Users\\kahna\\AppData\\Local\\Continuum\\anaconda3\\envs\\tutorials\\python.exe\n",
    "cachedir: .pytest_cache\n",
    "rootdir: C:\\Users\\kahna\\Documents\\Code\\python-intermediate-tutorials\\pytest_official_tutorial, inifile:\n",
    "collected 4 items / 3 deselected\n",
    "\n",
    "test_server.py::test_send_http PASSED                                    [100%]\n",
    "\n",
    "=================== 1 passed, 3 deselected in 0.02 seconds ===================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Additionally, the inverse can be selected to run tests that do not contain the keyword:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$ pytest -k \"not send_http\" -v\n",
    "============================= test session starts =============================\n",
    "platform win32 -- Python 3.7.1, pytest-3.9.1, py-1.7.0, pluggy-0.8.0 -- C:\\Users\\kahna\\AppData\\Local\\Continuum\\anaconda3\\envs\\tutorials\\python.exe\n",
    "cachedir: .pytest_cache\n",
    "rootdir: C:\\Users\\kahna\\Documents\\Code\\python-intermediate-tutorials\\pytest_official_tutorial\\4_marks, inifile: pytest.ini\n",
    "collected 4 items / 1 deselected\n",
    "\n",
    "test_server.py::test_something_quick PASSED                              [ 33%]\n",
    "test_server.py::test_another PASSED                                      [ 66%]\n",
    "test_server.py::TestClass::test_method PASSED                            [100%]\n",
    "\n",
    "=================== 3 passed, 1 deselected in 0.04 seconds ===================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To select the \"http\" and \"quick\" tests:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$ pytest -k \"http or quick\" -v\n",
    "============================= test session starts =============================\n",
    "platform win32 -- Python 3.7.1, pytest-3.9.1, py-1.7.0, pluggy-0.8.0 -- C:\\Users\\kahna\\AppData\\Local\\Continuum\\anaconda3\\envs\\tutorials\\python.exe\n",
    "cachedir: .pytest_cache\n",
    "rootdir: C:\\Users\\kahna\\Documents\\Code\\python-intermediate-tutorials\\pytest_official_tutorial\\4_marks, inifile: pytest.ini\n",
    "collected 4 items / 2 deselected\n",
    "\n",
    "test_server.py::test_send_http PASSED                                    [ 50%]\n",
    "test_server.py::test_something_quick PASSED                              [100%]\n",
    "\n",
    "=================== 2 passed, 2 deselected in 0.03 seconds ===================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Problem names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If using expressions like `\"X and Y\"`, then both `X` and `Y` need to be simple non-keyword names (like `pass` or `from`)\n",
    "\n",
    "But ff the `\"-k\"` argument is a simple string (no `and` combo modifier inside `\"\"`), then the restriction does not apply\n",
    "\n",
    "Also `\"-k 'not STRING'\"` has no restrictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Searching for parametrizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Use `\"-k 1.3\"` to match tests with the float \"1.3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Registering markers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[Registering markers](https://docs.pytest.org/en/latest/example/markers.html#registering-markers)\n",
    "\n",
    "Use `pytest.ini` to register markers for a test suite, with a description after the `:`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# content of pytest.ini\n",
    "[pytest]\n",
    "markers =\n",
    "    webtest: mark a test as a webtest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Note that this is similar to the `--strict` format mentioned in [Raising errors on unknown marks: –strict](#Raising-errors-on-unknown-marks:-–strict)\n",
    "\n",
    "To check which markers exist for the test suite, including the new `webtest` marker and the additional `slow` and `serial` markers from the above section, use `--markers`:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$ pytest --markers\n",
    "@pytest.mark.slow:\n",
    "\n",
    "@pytest.mark.serial:\n",
    "\n",
    "@pytest.mark.webtest: mark a test as a webtest\n",
    "\n",
    "@pytest.mark.filterwarnings(warning): add a warning filter to the given test. see https://docs.pytest.org/en/latest/warnings.html#pytest-mark-filterwarnings\n",
    "\n",
    "@pytest.mark.skip(reason=None): skip the given test function with an optional reason. Example: skip(reason=\"no way of currently testing this\") skips the test.\n",
    "\n",
    "@pytest.mark.skipif(condition): skip the given test function if eval(condition) results in a True value.  Evaluation happens within the module global context. Example: skipif('sys.platform ==\n",
    "\"win32\"') skips the test if we are on the win32 platform. see https://docs.pytest.org/en/latest/skipping.html\n",
    "\n",
    "@pytest.mark.xfail(condition, reason=None, run=True, raises=None, strict=False): mark the test function as an expected failure if eval(condition) has a True value. Optionally specify a reason\n",
    "for better reporting and run=False if you don't even want to execute the test function. If only\n",
    "specific exception(s) are expected, you can list them in raises, and if the test fails in other\n",
    "ways, it will be reported as a true failure. See https://docs.pytest.org/en/latest/skipping.html\n",
    "\n",
    "@pytest.mark.parametrize(argnames, argvalues): call a test function multiple times passing in different arguments in turn. argvalues generally needs to be a list of values if argnames specifies only one name or a list of tuples of values if argnames specifies multiple names. Example: @parametrize('arg1', [1,2]) would lead to two calls of the decorated test function, one with arg1=1 and another with arg1=2.see https://docs.pytest.org/en/latest/parametrize.html for more info and examples.\n",
    "\n",
    "@pytest.mark.usefixtures(fixturename1, fixturename2, ...): mark tests as needing all of the specified fixtures. see https://docs.pytest.org/en/latest/fixture.html#usefixtures\n",
    "\n",
    "@pytest.mark.tryfirst: mark a hook implementation function such that the plugin machinery will try to call it first/as early as possible.\n",
    "\n",
    "@pytest.mark.trylast: mark a hook implementation function such that the plugin machinery will try to call it last/as late as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "It is recommended that markers be registered in `pytest.ini` with `--strict` so that mistypes do not cause problems and so that `--markers` gives helpful output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Marking whole classes or modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[Marking whole classes or modules](https://docs.pytest.org/en/latest/example/markers.html#marking-whole-classes-or-modules)\n",
    "\n",
    "`pytest.mark` decorators can be applied to a class and will cascade to each test method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# content of test_mark_classlevel.py\n",
    "import pytest\n",
    "@pytest.mark.webtest\n",
    "class TestClass(object):\n",
    "    def test_startup(self):\n",
    "        pass\n",
    "    def test_startup_and_more(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is equivalent to marking each of the tests\n",
    "\n",
    "To remain backwards compatible with Python 2.4, the `pytestmark` attribute can also be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "class TestClass(object):\n",
    "    pytestmark = pytest.mark.webtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To set multiple markers, a list will suffice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "class TestClass(object):\n",
    "    pytestmark = [pytest.mark.webtest, pytest.mark.slowtest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A module level marker can also be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "pytestmark = pytest.mark.webtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this case, all functions and methods in the module will be marked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Marking individual tests when using parametrize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[Marking individual tests when using parametrize](https://docs.pytest.org/en/latest/example/markers.html#marking-individual-tests-when-using-parametrize)\n",
    "\n",
    "When using parametrize, a mark will apply to each individual test\n",
    "\n",
    "A mark can also be applied to an individual test instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "@pytest.mark.foo\n",
    "@pytest.mark.parametrize((\"n\", \"expected\"), [\n",
    "    (1, 2),\n",
    "    pytest.mark.bar((1, 3)),\n",
    "    (2, 3),\n",
    "])\n",
    "def test_increment(n, expected):\n",
    "     assert n + 1 == expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this case `foo` will apply to each of the 3 tests, but `bar` will only be applied to the second test (for which values are `(1, 3)`)\n",
    "\n",
    "`skip` and `xfail` marks can be applied in this way too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Working with single callables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If the parametrized data is a single callable, marking `pytest.xfail(my_func)` won't work\n",
    "\n",
    "Instead, use `pytest.mark.xfail(func_bar, reason=\"Issue#7)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Custom marker and command line option to control test runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[Custom marker and command line option to control test runs](https://docs.pytest.org/en/latest/example/markers.html#custom-marker-and-command-line-option-to-control-test-runs)\n",
    "\n",
    "Plugins can provide custom markers and extra behavior\n",
    "\n",
    "The example below adds a command line option and a parametrized test function marker to run tests specified via named environments\n",
    "\n",
    "The below config file is used for the next few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# content of conftest.py\n",
    "\n",
    "import pytest\n",
    "def pytest_addoption(parser):\n",
    "    parser.addoption(\"-E\", action=\"store\", metavar=\"NAME\",\n",
    "        help=\"only run tests matching the environment NAME.\")\n",
    "\n",
    "def pytest_configure(config):\n",
    "    # register an additional marker\n",
    "    config.addinivalue_line(\"markers\",\n",
    "        \"env(name): mark test to run only on named environment\")\n",
    "\n",
    "def pytest_runtest_setup(item):\n",
    "    envnames = [mark.args[0] for mark in item.iter_markers(name='env')]\n",
    "    if envnames:\n",
    "        if item.config.getoption(\"-E\") not in envnames:\n",
    "            pytest.skip(\"test requires env in %r\" % envnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A test file that uses this local plugin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# content of test_someenv.py\n",
    "\n",
    "import pytest\n",
    "@pytest.mark.env(\"stage1\")\n",
    "def test_basic_db_operation():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "An example invocation that specifies a different environment than what the test needs:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$ pytest test_someenv.py -E stage2\n",
    "============================= test session starts =============================\n",
    "platform win32 -- Python 3.7.1, pytest-3.9.1, py-1.7.0, pluggy-0.8.0\n",
    "rootdir: C:\\Users\\kahna\\Documents\\Code\\python-intermediate-tutorials\\pytest_official_tutorial\\4_marks, inifile: pytest.ini\n",
    "collected 1 item\n",
    "\n",
    "test_someenv.py s                                                        [100%]\n",
    "\n",
    "========================== 1 skipped in 0.02 seconds =========================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the below case, now the `stage1` specifications is used as needed:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$ pytest test_someenv.py -E stage1\n",
    "============================= test session starts =============================\n",
    "platform win32 -- Python 3.7.1, pytest-3.9.1, py-1.7.0, pluggy-0.8.0\n",
    "rootdir: C:\\Users\\kahna\\Documents\\Code\\python-intermediate-tutorials\\pytest_official_tutorial\\4_marks, inifile: pytest.ini\n",
    "collected 1 item\n",
    "\n",
    "test_someenv.py .                                                        [100%]\n",
    "\n",
    "========================== 1 passed in 0.02 seconds ==========================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Again the `--markers` option gives a list of available markers. See @`pytest.mark.env(name)`:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$ pytest --markers\n",
    "@pytest.mark.slow:\n",
    "\n",
    "@pytest.mark.serial:\n",
    "\n",
    "@pytest.mark.webtest: mark a test as a webtest\n",
    "\n",
    "@pytest.mark.env(name): mark test to run only on named environment\n",
    "\n",
    "@pytest.mark.filterwarnings(warning): add a warning filter to the given test. see https://docs.pytest.org/en/latest/warnings.html#pytest-mark-filterwarnings\n",
    "\n",
    "@pytest.mark.skip(reason=None): skip the given test function with an optional reason. Example: skip(reason=\"no way of currently testing this\") skips the test.\n",
    "\n",
    "@pytest.mark.skipif(condition): skip the given test function if eval(condition) results in a True value.  Evaluation happens within the module global context. Example: skipif('sys.platform == \"win32\"') skips the test if we are on the win32 platform. see https://docs.pytest.org/en/latest/skipping.html\n",
    "\n",
    "@pytest.mark.xfail(condition, reason=None, run=True, raises=None, strict=False):\n",
    "mark the test function as an expected failure if eval(condition) has a True value. Optionally specify a reason for better reporting and run=False if you don't even want to execute the test function. If only specific exception(s) are expected,\n",
    "you can list them in raises, and if the test fails in other ways, it will be reported as a true failure. See https://docs.pytest.org/en/latest/skipping.html\n",
    "\n",
    "@pytest.mark.parametrize(argnames, argvalues): call a test function multiple times passing in different arguments in turn. argvalues generally needs to be a list\n",
    "of values if argnames specifies only one name or a list of tuples of values if argnames specifies multiple names. Example: @parametrize('arg1', [1,2]) would lead\n",
    "to two calls of the decorated test function, one with arg1=1 and another with arg1=2.see https://docs.pytest.org/en/latest/parametrize.html for more info and examples.\n",
    "\n",
    "@pytest.mark.usefixtures(fixturename1, fixturename2, ...): mark tests as needing\n",
    "all of the specified fixtures. see https://docs.pytest.org/en/latest/fixture.html#usefixtures\n",
    "\n",
    "@pytest.mark.tryfirst: mark a hook implementation function such that the plugin machinery will try to call it first/as early as possible.\n",
    "\n",
    "@pytest.mark.trylast: mark a hook implementation function such that the plugin machinery will try to call it last/as late as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Passing a callable to custom markers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The next several examples use the below config file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# content of conftest.py\n",
    "import sys\n",
    "\n",
    "def pytest_runtest_setup(item):\n",
    "    for marker in item.iter_markers(name='my_marker'):\n",
    "        print(marker)\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A custom marker can have an argument set with `args` and `kwargs`, defined by either invoking the marker as a callable or by using `pytest.mark.MARKER_NAME.with_args`\n",
    "\n",
    "Usually the two methods achieve the same effect\n",
    "\n",
    "However, if there is a single callable as the single positional argument without keyword arguments, use the `pytest.mark.MARKER_NAME(c)` will not pass `c` as a positional argument, and will instead decorate `c` with the custom marker\n",
    "\n",
    "In this case, use `pytest.mark.MARKER_NAME.with_args`\n",
    "\n",
    "It is recommended thus to always use `pytest.mark.MARKER_NAME.with_args`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# content of test_custom_marker.py\n",
    "import pytest\n",
    "\n",
    "def hello_world(*args, **kwargs):\n",
    "    return 'Hello World'\n",
    "\n",
    "@pytest.mark.my_marker.with_args(hello_world)\n",
    "def test_with_args():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The output is as follows:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$ pytest -s -q test_custom_marker.py\n",
    "Mark(name='my_marker', args=(<function hello_world at 0x0000016E1FD8B048>,), kwargs={})\n",
    ".\n",
    "1 passed in 0.01 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this case, the custom marker has its argument set extended wth the function `hello_world`\n",
    "\n",
    "This is the key differene between creating a custom marker as a callable, which invokes `__call__` behind the scenes using `with_args`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Reading markers which were set from multiple places"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[Reading markers which were set from multiple places](https://docs.pytest.org/en/latest/example/markers.html#reading-markers-which-were-set-from-multiple-places)\n",
    "\n",
    "There may be a case where a marker is applied several times to a test function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# content of test_mark_three_times.py\n",
    "import pytest\n",
    "pytestmark = pytest.mark.glob(\"module\", x=1)\n",
    "\n",
    "@pytest.mark.glob(\"class\", x=2)\n",
    "class TestClass(object):\n",
    "    @pytest.mark.glob(\"function\", x=3)\n",
    "    def test_something(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this case, the `glob` mark has been applied 3 times to the `test_something` function\n",
    "\n",
    "A `conftest` can read this via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# content of conftest.py\n",
    "import sys\n",
    "\n",
    "def pytest_runtest_setup(item):\n",
    "    for mark in item.iter_markers(name='glob'):\n",
    "        print (\"glob args=%s kwargs=%s\" %(mark.args, mark.kwargs))\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Without capturing output:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$ pytest -q -s test_mark_three_times.py\n",
    "glob args=('function',) kwargs={'x': 3}\n",
    "glob args=('class',) kwargs={'x': 2}\n",
    "glob args=('module',) kwargs={'x': 1}\n",
    ".\n",
    "1 passed in 0.03 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Marking platform specific tests with pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[Marking platform specific tests with pytest](https://docs.pytest.org/en/latest/example/markers.html#marking-platform-specific-tests-with-pytest)\n",
    "\n",
    "Consider a test suite that marks platforms like `pytest.mark.darwin` or `pytest.mark.win32`, and has tests that run all all platforms\n",
    "\n",
    "The following plugin allows tests to only run for a specific platform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# content of conftest.py\n",
    "import sys\n",
    "import pytest\n",
    "\n",
    "ALL = set(\"darwin linux win32\".split())\n",
    "\n",
    "def pytest_runtest_setup(item):\n",
    "    supported_platforms = \\\n",
    "        ALL.intersection(mark.name for mark in item.iter_markers())\n",
    "    plat = sys.platform\n",
    "    if supported_platforms and plat not in supported_platforms:\n",
    "        pytest.skip(\"cannot run on platform %s\" % (plat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Tests will be skipped if they were specified for a different platform\n",
    "\n",
    "A test file that contains platform-specified tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# content of test_plat.py\n",
    "\n",
    "import pytest\n",
    "\n",
    "@pytest.mark.darwin\n",
    "def test_if_apple_is_evil():\n",
    "    pass\n",
    "\n",
    "@pytest.mark.linux\n",
    "def test_if_linux_works():\n",
    "    pass\n",
    "\n",
    "@pytest.mark.win32\n",
    "def test_if_win32_crashes():\n",
    "    pass\n",
    "\n",
    "def test_runs_everywhere():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Since the test was conducted on a Windows machine, the first two tests were skipped and the last two tests were conducted, denoted by `ss..`:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$ pytest -q test_plat.py\n",
    "ss..                                                                     [100%]\n",
    "2 passed, 2 skipped in 0.02 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A specific marker can be chosen to run as well:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$ pytest -q test_plat.py -m win32\n",
    ".                                                                        [100%]\n",
    "1 passed, 3 deselected in 0.01 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this case, only `win32` tests were selected, and the test was allowed to run because the plugin checks that the test corresponds to the current system platform\n",
    "\n",
    "However, if another system marker is used, then the plugin will prevent the test from running:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$ pytest -q test_plat.py -m linux\n",
    "s                                                                        [100%]\n",
    "1 skipped, 3 deselected in 0.01 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Automatically adding markes based on test names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If function names indicate a certain type of test, a hook can automatically define markers via the `-m` option\n",
    "\n",
    "Consider the test module below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# content of test_module.py\n",
    "\n",
    "def test_interface_simple():\n",
    "    assert 0\n",
    "\n",
    "def test_interface_complex():\n",
    "    assert 0\n",
    "\n",
    "def test_event_simple():\n",
    "    assert 0\n",
    "\n",
    "def test_something_else():\n",
    "    assert 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A `conftest.py` plugin can then dynamically define two markers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# content of conftest.py\n",
    "\n",
    "import pytest\n",
    "def pytest_collection_modifyitems(items):\n",
    "    for item in items:\n",
    "        if \"interface\" in item.nodeid:\n",
    "            item.add_marker(pytest.mark.interface)\n",
    "        elif \"event\" in item.nodeid:\n",
    "            item.add_marker(pytest.mark.event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now the -m option can select one set:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$ pytest test_module.py -q -m interface --tb=short\n",
    "FF                                                                       [100%]\n",
    "================================== FAILURES ===================================\n",
    "____________________________ test_interface_simple ____________________________\n",
    "test_module.py:2: in test_interface_simple\n",
    "    assert 0\n",
    "E   assert 0\n",
    "___________________________ test_interface_complex ____________________________\n",
    "test_module.py:6: in test_interface_complex\n",
    "    assert 0\n",
    "E   assert 0\n",
    "2 failed, 2 deselected in 0.03 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To select both `event` and `interface` tests:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$ pytest test_module.py -q -m \"interface or event\" --tb=short\n",
    "FFF                                                                      [100%]\n",
    "================================== FAILURES ===================================\n",
    "____________________________ test_interface_simple ____________________________\n",
    "test_module.py:2: in test_interface_simple\n",
    "    assert 0\n",
    "E   assert 0\n",
    "___________________________ test_interface_complex ____________________________\n",
    "test_module.py:6: in test_interface_complex\n",
    "    assert 0\n",
    "E   assert 0\n",
    "______________________________ test_event_simple ______________________________\n",
    "test_module.py:10: in test_event_simple\n",
    "    assert 0\n",
    "E   assert 0\n",
    "3 failed, 1 deselected in 0.03 seconds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "54px",
    "width": "328px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
